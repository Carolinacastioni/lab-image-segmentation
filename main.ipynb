{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Before your start:\n",
    "\n",
    "   - Read the README.md file\n",
    "   - Comment as much as you can and use the resources in the README.md file\n",
    "   - Happy learning!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image Segmentation** is an image analysis task in which we classify each pixel in the image into a class. This is similar to what us humans do all the time by default. \n",
    "\n",
    "What’s the first thing you do when you’re attempting to cross the road?\n",
    "\n",
    "We typically look left and right, take stock of the vehicles on the road, and make our decision. Our brain is able to analyze, in a matter of milliseconds, what kind of vehicle (car, bus, truck, auto, etc.) is coming towards us. Whenever we are looking at something, then we try to “segment” what portion of the image belongs to which class/label/category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](images/test.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So how does image segmentation work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can divide or partition the image into various parts called segments. It’s not a great idea to process the entire image at the same time as there will be regions in the image which do not contain any information. By dividing the image into segments, we can make use of the important segments for processing the image. That, in a nutshell, is how image segmentation works.\n",
    "\n",
    "An image is a collection or set of different pixels. We group together the pixels that have similar attributes using image segmentation. \n",
    "\n",
    "**Sooner or later all things are numbers, including images.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/58/668ffb25215b3f8231a550a227be7f905f514859c70a65ca59d28f9b7f60/torch-1.5.0-cp37-cp37m-manylinux1_x86_64.whl (752.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 752.0MB 2.2kB/s  eta 0:00:01  5% |██                              | 44.9MB 21.8MB/s eta 0:00:33    7% |██▌                             | 59.1MB 9.0MB/s eta 0:01:18    9% |███                             | 69.2MB 33.6MB/s eta 0:00:21    9% |███▏                            | 73.5MB 15.3MB/s eta 0:00:45    13% |████▍                           | 102.2MB 25.6MB/s eta 0:00:26    14% |████▌                           | 106.0MB 21.9MB/s eta 0:00:30    15% |█████                           | 118.4MB 25.8MB/s eta 0:00:25    16% |█████▍                          | 125.5MB 29.3MB/s eta 0:00:22    23% |███████▌                        | 175.5MB 28.7MB/s eta 0:00:21    28% |█████████                       | 213.7MB 24.8MB/s eta 0:00:22    29% |█████████▋                      | 225.2MB 10.0MB/s eta 0:00:53    30% |█████████▊                      | 229.3MB 28.2MB/s eta 0:00:19    30% |██████████                      | 233.1MB 27.2MB/s eta 0:00:20    36% |███████████▉                    | 278.1MB 25.1MB/s eta 0:00:19    38% |████████████▎                   | 289.5MB 22.4MB/s eta 0:00:21    39% |████████████▋                   | 295.4MB 21.6MB/s eta 0:00:22    39% |████████████▊                   | 298.5MB 22.2MB/s eta 0:00:21    39% |████████████▊                   | 299.8MB 24.3MB/s eta 0:00:19    42% |█████████████▊                  | 323.3MB 19.9MB/s eta 0:00:22    43% |█████████████▉                  | 324.8MB 20.3MB/s eta 0:00:22    44% |██████████████▏                 | 333.3MB 39.6MB/s eta 0:00:11    44% |██████████████▍                 | 337.9MB 18.2MB/s eta 0:00:23    49% |████████████████                | 374.6MB 16.3MB/s eta 0:00:24    50% |████████████████                | 376.1MB 38.0MB/s eta 0:00:10    52% |████████████████▊               | 393.3MB 17.3MB/s eta 0:00:21    53% |█████████████████               | 399.0MB 20.4MB/s eta 0:00:18    57% |██████████████████▌             | 435.6MB 30.4MB/s eta 0:00:11    62% |████████████████████            | 469.9MB 31.6MB/s eta 0:00:09    62% |████████████████████            | 472.5MB 29.3MB/s eta 0:00:10    64% |████████████████████▌           | 481.5MB 20.1MB/s eta 0:00:14    64% |████████████████████▋           | 484.4MB 26.9MB/s eta 0:00:10    76% |████████████████████████▌       | 576.6MB 11.2MB/s eta 0:00:16    77% |████████████████████████▉       | 582.6MB 29.9MB/s eta 0:00:06    80% |█████████████████████████▊      | 605.4MB 18.8MB/s eta 0:00:08    81% |██████████████████████████      | 610.0MB 27.1MB/s eta 0:00:06    85% |███████████████████████████▎    | 640.1MB 15.3MB/s eta 0:00:08    87% |████████████████████████████    | 659.5MB 25.4MB/s eta 0:00:04    91% |█████████████████████████████▍  | 691.3MB 28.6MB/s eta 0:00:03    92% |█████████████████████████████▌  | 692.6MB 24.0MB/s eta 0:00:03    94% |██████████████████████████████▏ | 710.2MB 45.0MB/s eta 0:00:01    97% |███████████████████████████████▍| 736.8MB 17.9MB/s eta 0:00:01    98% |███████████████████████████████▋| 743.2MB 26.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting future (from torch)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
      "\u001b[K    100% |████████████████████████████████| 829kB 219kB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/lib/python3/dist-packages (from torch) (1.16.2)\n",
      "Building wheels for collected packages: future\n",
      "  Running setup.py bdist_wheel for future ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/carolina/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
      "Successfully built future\n",
      "Installing collected packages: future, torch\n",
      "Successfully installed future-0.18.2 torch-1.5.0\n",
      "Collecting torchvision\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/ed/a894f274a7733d6492e438a5831a95b507c5ec777edf6d8c3b97574e08c4/torchvision-0.6.0-cp37-cp37m-manylinux1_x86_64.whl (6.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 6.6MB 254kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /usr/lib/python3/dist-packages (from torchvision) (6.1.0)\n",
      "Requirement already satisfied: numpy in /usr/lib/python3/dist-packages (from torchvision) (1.16.2)\n",
      "Requirement already satisfied: torch==1.5.0 in /home/carolina/.local/lib/python3.7/site-packages (from torchvision) (1.5.0)\n",
      "Requirement already satisfied: future in /home/carolina/.local/lib/python3.7/site-packages (from torch==1.5.0->torchvision) (0.18.2)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.6.0\n",
      "Requirement already satisfied: pillow in /usr/lib/python3/dist-packages (6.1.0)\n"
     ]
    }
   ],
   "source": [
    "# Install torch & torchvision, it might take some time\n",
    "\n",
    "!pip3 install torch\n",
    "!pip3 install torchvision\n",
    "!pip3 install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /home/carolina/.cache/torch/checkpoints/resnet101-5d3b4d8f.pth\n",
      "13.8%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "36.2%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "58.4%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.0%\n",
      "Downloading: \"https://download.pytorch.org/models/fcn_resnet101_coco-7ecb50ca.pth\" to /home/carolina/.cache/torch/checkpoints/fcn_resnet101_coco-7ecb50ca.pth\n",
      "58.3%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "73.6%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "89.4%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import your libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import scipy.signal\n",
    "from imageProcessing import resizeImg, generateMask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1 - Load a picture & let the magic begin!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to segment an image, what **we first need is an image**. So, first thing you should do is loading one of yourself. Use [`Image.open`](https://pillow.readthedocs.io/en/3.1.x/reference/Image.html) for this purpose and call `img` to this variable. (In case you don't want to use your own image you can use one located at `images/bird.png`.\n",
    "\n",
    "Using this function you will be using Pillow, the most popular and de facto standard library in Python for loading and working with image data. However, if you'd like to see how gorgeous you look on that picture, try using [`plt.show`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.show.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is desirable to thumbnail all images to have the same width or height. In this case, the width, will be reduced to 256, using `resizeImg`, and the height will be scaled in order to retain the aspect ratio of the image.\n",
    "\n",
    "*Hint: use `img.size` as much as you need until you get a size of (384, 256)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, don't worry! This time won't be that difficult, you get some code to make your life easier.\n",
    "Here the function `generateMask`, as you may imagine, will generate a [`mask`](http://www.xinapse.com/Manual/masking.html) for the picture you loaded. One for the background of the picture, one for the foreground. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a mask\n",
    "\n",
    "mask = generateMask(img)\n",
    "background_mask = np.all(mask == [0, 0, 0], axis=-1)\n",
    "foreground_mask = np.any(mask != [0, 0, 0], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn again, convert the image to np.array and apply the mask twice to the picture, once to the `foreground` and once to the `background`. Make a couple of copies of the image instead of assigning the same value to different variables. After doing this, you may want to check how those are looking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2 - Two 2-dimensional arrays, one channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution involving one-dimensional signals is referred to as 1D convolution or just convolution. Otherwise, if the convolution is performed between two signals spanning along two mutually perpendicular dimensions (i.e., if signals are two-dimensional in nature), then it will be referred to as [`2D convolution`](http://www.songho.ca/dsp/convolution/convolution2d_example.html). \n",
    "\n",
    "This kind of operation is extensively used in the field of digital image processing, such as smoothing, sharpening, and edge detection of images. Wherein the 2D matrix representing the image will be convolved with a comparatively smaller matrix called 2D kernel. \n",
    "\n",
    "[*What do you mean by one channel?*](https://brohrer.github.io/convert_rgb_to_grayscale.html)\n",
    "\n",
    "Color images are represented as three-dimensional Numpy arrays - a collection of three two-dimensional arrays, one each for red, green, and blue channels. Each one, like grayscale arrays, has one value per pixel and their ranges are identical. \n",
    "\n",
    "An intuitive way to convert a color image 3D array to a grayscale 2D array is, for each pixel, take the average of the red, green, and blue pixel values to get the grayscale value. This combines the lightness or luminance contributed by each color band into a reasonable gray approximation. \n",
    "\n",
    "<img src=\"images/perritorgb.png\" width=\"500\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First let's begin with black and white (only one channel)\n",
    "\n",
    "Transform `foreground` and `background`, now they are arrays, to an image and convert them in gray scale. Name they `fore_bnw` and `back_bnw` respectively.\n",
    "\n",
    "*Hint: use `.fromarray` and then `.convert`*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show `fore_bnw` and `back_bnw` in order to check that they are in a gray scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transform `back_bnw` into an array and name it `back_np`.\n",
    "\n",
    "And create a 2d matrix with random values of size 20x20 uniformly distributed and name it `kernel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you must use `signal.convolve2d` from scipy and pass the array image (`back_np`) and the kernel as parameters. [Check the docs for `convolve2d`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html) and assign it to a variable called `conv`.\n",
    "\n",
    "*Hint: use mode='same' in order to get an array with the same size as the input one.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you must operate with `conv` in order to have their values between 0 and 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply `.absolute` from numpy to the new value of `conv` and assign it to `back_blur`, and if you show this `back_blur` you will notice that the blur effect has been apply to the background.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, create a new variable called `final` which have `back_blur` and `fore_bnw` arrays using the `.add` function.\n",
    "\n",
    "Make sure the type of the output is `np.uint8`, else use the `.astype` function. Show `final`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3 -  Once wasn't enough, do it three times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**aquí es donde la matan**\n",
    "-Marc Pomar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a `def blur(img): ...` function which blurs an image having RGB channels. To do this:\n",
    "1. Using numpy generate a 2d matrix with random values of size 20x20 uniformly distributed. We call this the `kernel`.  \n",
    "2. Use `signal.convolve2d` from scipy and pass the image and the kernel as parameters.[Check the docs for `convolve2d`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html)\n",
    "3. Check the output values and ensure data is in range 0-255. *Hint: Divide the output array by max value.*\n",
    "\n",
    "Make sure the type of the output is `np.uint8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have demonstrated you are a great coder defining the `blur` function. It's time for you to check if it works, bluring the background of the `background` image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the `foreground_mask` again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum the blured background and the original foreground using [`np.add`](https://numpy.org/doc/1.18/reference/generated/numpy.add.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, once you've got something like the picture below, please, export that **Image**. Make sure you add this result and all of those you generate to the Pull Request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/bird.png\" width=\"300\">\n",
    "<img src=\"images/blured_bird.JPG\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enjoy your weekend!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
